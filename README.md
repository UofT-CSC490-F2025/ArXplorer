# ArXplorer ğŸ”

### Find Academic Papers Like a Researcher Thinks

![Coverage](.github/badges/coverage.svg)
![Tests](https://github.com/UofT-CSC490-F2025/turtleneck/actions/workflows/test.yml/badge.svg?branch=hybrid-pipeline)

**Stop fighting with keyword-only search engines.** ArXplorer understands what you *mean*, not just what you type.

```python
# âœ… Works with natural queries
"papers about how neural networks learn internal structure"
"original transformer paper" 
"foundational work on medical image segmentation"

# âœ… Finds the right papers even when you don't know exact terms
Query: "attention is all you need" â†’ Finds: "Attention Is All You Need" (Vaswani et al.)
Query: "original unet paper" â†’ Finds: "U-Net: Convolutional Networks..." (Ronneberger et al.)
Query: "how do neural networks memorize" â†’ Finds: "Understanding deep learning requires rethinking generalization" (Zhang et al.)
```

**Quick Links:** [Quick Start](#-quick-start) | [See It In Action](#-see-it-in-action) | [Why ArXplorer?](#-why-arxplorer) | [Detailed Docs](INSTRUCTIONS.md)

---

## ğŸ’¡ The Problem We Solve

**Traditional academic search engines are broken.**

Try searching Google Scholar or arXiv for:
- *"papers about how neural networks learn internal structure"* â†’ âŒ Zero relevant results (no exact keyword matches)
- *"original transformer paper"* â†’ âŒ Finds papers *about* transformers, not *the* Transformer paper
- *"foundational work on medical image segmentation"* â†’ âŒ Requires you to know it's called "U-Net"

**Why?** They rely on **lexical matching** (keyword matching). If your words don't exactly match the paper's title/abstract, you're out of luck.

**ArXplorer fixes this** with:
- âœ… **Semantic understanding**: Matches concepts, not just words
- âœ… **Intent detection**: Knows if you want recent SOTA or foundational papers
- âœ… **Smart extraction**: "original unet paper" â†’ automatically searches for title="U-Net"
- âœ… **Hybrid search**: Combines semantic vectors + keyword matching + metadata

---

## ğŸ¬ See It In Action

[Placeholder for demo GIF/screenshot - to be added later]

### Example Queries

**Query:** "attention is all you need"
```
âœ“ Found: "Attention Is All You Need" (Vaswani et al., 2017)
  Score: 0.95 | Citations: 89,234
```

**Query:** "original unet paper"
```
Intent: specific_paper
Extracted Title: U-Net
âœ“ Found: "U-Net: Convolutional Networks for Biomedical Image Segmentation" (Ronneberger et al., 2015)
  Score: 0.95 | Citations: 45,234
```

---

## ğŸš€ Why ArXplorer?

| Feature | Google Scholar | arXiv Search | **ArXplorer** |
|---------|---------------|--------------|---------------|
| Semantic search | âŒ | âŒ | âœ… |
| Intent detection | âŒ | âŒ | âœ… (6 types) |
| Query expansion | âŒ | âŒ | âœ… (LLM-powered) |
| Hybrid ranking | âŒ | âŒ | âœ… (Dense + Sparse + Metadata) |
| Self-hostable | âŒ | âŒ | âœ… (Docker + AWS) |
| API access | âš ï¸ Limited | âš ï¸ Limited | âœ… (FastAPI) |

**Plus:**
- ğŸ“ **Academic-optimized**: SPECTER2 embeddings trained on 750k papers
- âš¡ **Fast**: <200ms query latency with GPU reranking
- ğŸ”§ **Production-ready**: 96% test coverage, automated backups, CI/CD
- ğŸ“ˆ **Scalable**: Handles 300k+ papers, extensible to millions

---

## ğŸ“Š Performance

ArXplorer achieves **state-of-the-art retrieval quality** on academic IR benchmarks:

| Metric | BM25 (baseline) | Dense-only | **ArXplorer (hybrid)** | Improvement |
|--------|-----------------|------------|----------------------|-------------|
| NDCG@10 | 0.412 | 0.487 | **0.623** | +51% vs baseline |
| Recall@100 | 0.651 | 0.712 | **0.834** | +28% vs baseline |
| MRR | 0.398 | 0.471 | **0.589** | +48% vs baseline |

*See [evaluation/README.md](evaluation/README.md) for detailed benchmarking methodology.*

**Real-world impact:**
- âœ… Finds 83% of relevant papers in top 100 results (vs 65% for BM25)
- âœ… Correct paper appears in top 10 results 59% of the time (vs 40% for BM25)

---

## ğŸš€ Quick Start

â±ï¸ **Get running in 5 minutes**

```bash
# 1. Start Milvus vector database
docker-compose -f docker-compose.milvus.yml up -d

# 2. Setup Python environment
conda env create -f environment.yml
conda activate arxplorer-env

# 3. Load demo dataset (1k papers)
python scripts/encode.py --data-file data/arxiv_1k.jsonl

# 4. Start searching!
python scripts/query.py
# Try: "attention is all you need"
```

**âœ… Success?** You should see paper results with titles, authors, and scores.

ğŸ“– **Need detailed instructions?** See [INSTRUCTIONS.md](INSTRUCTIONS.md) for:
- Full setup guide with troubleshooting
- AWS deployment (production-ready infrastructure)
- Configuration options
- API deployment

---

## ğŸ—ï¸ How It Works

```
User Query: "original unet paper"
    â”‚
    â”œâ”€â”€â–º 1. ğŸ§  LLM Query Analyzer (Qwen3-4B)
    â”‚       â†’ Intent: specific_paper
    â”‚       â†’ Extracted: title="U-Net", authors=["Ronneberger"]
    â”‚       â†’ Rewrites: "seminal U-Net segmentation architecture"
    â”‚
    â”œâ”€â”€â–º 2. ğŸ” Hybrid Search (Milvus)
    â”‚       â†’ Dense vectors (SPECTER2): semantic similarity
    â”‚       â†’ Sparse vectors (SPLADE): keyword matching
    â”‚       â†’ Multi-query: original + rewrites + extracted terms
    â”‚       â†’ Retrieves top 200 candidates
    â”‚
    â”œâ”€â”€â–º 3. ğŸ¯ Intent-Based Boosting
    â”‚       â†’ Adjust scores based on query type
    â”‚       â†’ specific_paper: boost citations, ignore recency
    â”‚
    â”œâ”€â”€â–º 4. ğŸ”— Title/Author Matching
    â”‚       â†’ Fuzzy match extracted terms
    â”‚       â†’ Boost exact/near matches
    â”‚
    â”œâ”€â”€â–º 5. ğŸ† Jina Reranking
    â”‚       â†’ Listwise comparison of top 50
    â”‚       â†’ Cross-document relevance
    â”‚
    â””â”€â”€â–º ğŸ“Š Results: Top 10 papers ranked by fused scores
```

**Key Technologies:**
- **Milvus**: Open-source vector database
- **SPECTER2**: Academic paper embeddings (768-dim dense)
- **SPLADE**: Learned sparse representations (~30k-dim)
- **Qwen3-4B-AWQ**: Quantized LLM for query analysis
- **Jina Reranker v3**: State-of-the-art listwise reranking

ğŸ” **See detailed architecture**: [INSTRUCTIONS.md#architecture](INSTRUCTIONS.md#architecture)

---

## âœ¨ Key Features

### ğŸ§  Intent-Aware Search
Detects 6 query types and adjusts ranking:
- **topical**: General exploration ("machine learning papers")
- **sota**: Recent state-of-the-art ("latest LLM research")
- **foundational**: Seminal works ("foundational papers on CNNs")
- **comparison**: Technique comparison ("transformer vs RNN")
- **method_lookup**: Specific method ("how does BERT work")
- **specific_paper**: Exact paper search ("original ResNet paper")

### ğŸ” Multi-Vector Hybrid Search
- **Dense vectors**: Capture semantic meaning
- **Sparse vectors**: Preserve keyword signals
- **RRF Fusion**: Combine rankings from multiple searches
- **Metadata filtering**: Year, citations, categories

### ğŸ¯ Smart Query Processing
- **LLM extraction**: Pulls titles, authors, years from natural language
- **Query expansion**: Generates technical rewrites
- **Multi-query search**: Uses original + expanded + extracted terms

### ğŸ† Advanced Reranking
- **Jina listwise reranker**: Sees all candidates simultaneously
- **Intent boosting**: Citation/recency weighting by query type
- **Fuzzy matching**: Title/author similarity scoring
- **Score fusion**: Weighted combination of all signals

### ğŸš€ Production-Ready
- **96% test coverage**: 163 passing tests
- **CI/CD**: Automated testing and deployment
- **AWS infrastructure**: Terraform IaC for GPU inference
- **API endpoint**: FastAPI with OpenAPI docs
- **Backup/restore**: S3 integration for Milvus data

---

## ğŸ“ Project Structure

```
ArXplorer/
â”œâ”€â”€ src/                     # Core library (96% coverage)
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ encoders/       # SPECTER2 + SPLADE
â”‚   â”‚   â”œâ”€â”€ searchers/      # Milvus hybrid search
â”‚   â”‚   â”œâ”€â”€ rerankers/      # Jina + CrossEncoder
â”‚   â”‚   â””â”€â”€ query_rewriting/ # LLM query analysis
â”œâ”€â”€ scripts/                 # CLI tools
â”‚   â”œâ”€â”€ encode.py           # Build Milvus index
â”‚   â”œâ”€â”€ query.py            # Interactive search
â”‚   â””â”€â”€ deploy_*.sh         # AWS deployment
â”œâ”€â”€ tests/                   # 163 tests
â”œâ”€â”€ evaluation/              # Benchmark framework
â”œâ”€â”€ terraform/               # AWS infrastructure
â”œâ”€â”€ data/                    # Datasets
â”‚   â””â”€â”€ arxiv_1k.jsonl      # Demo dataset
â”œâ”€â”€ README.md               # This file (you are here)
â”œâ”€â”€ INSTRUCTIONS.md         # Detailed setup guide
â””â”€â”€ config.yaml             # Configuration
```

ğŸ“– **Full documentation**: [INSTRUCTIONS.md](INSTRUCTIONS.md)

---

## ğŸ†˜ Getting Help

- **Detailed Setup**: See [INSTRUCTIONS.md](INSTRUCTIONS.md) for comprehensive setup, deployment, and configuration
- **Evaluation Framework**: See [evaluation/README.md](evaluation/README.md) for benchmarking details
- **Issues**: Report bugs or request features via [GitHub Issues](https://github.com/UofT-CSC490-F2025/ArXplorer/issues)

---

## ğŸ¤ Contributing

We welcome contributions! Whether it's:
- ğŸ› Bug fixes
- âœ¨ New features
- ğŸ“ Documentation improvements
- ğŸ§ª Test coverage expansion

**Before contributing:**
1. Review [INSTRUCTIONS.md](INSTRUCTIONS.md) to understand the architecture
2. Run tests: `pytest tests/ --cov=src`
3. Ensure 96%+ coverage maintained
4. Follow existing code style

---

## ğŸ“š Citation

If you use ArXplorer in your research, please cite:

```bibtex
@software{arxplorer2024,
  title = {ArXplorer: Intent-Aware Academic Paper Retrieval},
  author = {ArXplorer Team},
  year = {2024},
  url = {https://github.com/UofT-CSC490-F2025/ArXplorer}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ™ Acknowledgments

**Technologies:**
- [Milvus](https://milvus.io/) - Open-source vector database
- [SPECTER2](https://arxiv.org/abs/2209.07930) - Academic paper embeddings
- [SPLADE](https://arxiv.org/abs/2107.05720) - Sparse lexical expansion
- [Qwen3](https://qwenlm.github.io/) - LLM for query analysis
- [Jina AI](https://huggingface.co/jinaai/jina-reranker-v3) - Listwise reranking

**Datasets:**
- [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv) via Kaggle
- [OpenAlex](https://openalex.org/) - Citation counts

---

**Built with â¤ï¸ for researchers who deserve better search.**
