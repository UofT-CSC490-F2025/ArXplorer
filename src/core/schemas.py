"""
Data schemas for ArXplorer - Academic Search Assistant for arXiv
Defines data models for papers, embeddings, and metadata processing
"""

from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any
from datetime import datetime
from enum import Enum
import json


class PaperStatus(Enum):
    """Status of paper processing in the pipeline"""
    RAW = "raw"
    PROCESSED = "processed" 
    EMBEDDED = "embedded"
    INDEXED = "indexed"
    FAILED = "failed"


@dataclass
class Author:
    """Author information schema"""
    name: str
    affiliation: Optional[str] = None
    email: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "affiliation": self.affiliation,
            "email": self.email
        }


@dataclass 
class ArXivPaper:
    """Core arXiv paper schema for data ingestion and storage"""
    arxiv_id: str  # e.g., "2301.12345"
    title: str
    abstract: str
    authors: List[Author]
    categories: List[str]  # arXiv categories like "cs.AI", "stat.ML"
    submitted_date: datetime
    updated_date: Optional[datetime] = None
    doi: Optional[str] = None
    journal_ref: Optional[str] = None
    comments: Optional[str] = None
    
    # Processing metadata
    status: PaperStatus = PaperStatus.RAW
    processed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage"""
        return {
            "arxiv_id": self.arxiv_id,
            "title": self.title,
            "abstract": self.abstract,
            "authors": [author.to_dict() for author in self.authors],
            "categories": self.categories,
            "submitted_date": self.submitted_date.isoformat(),
            "updated_date": self.updated_date.isoformat() if self.updated_date else None,
            "doi": self.doi,
            "journal_ref": self.journal_ref,
            "comments": self.comments,
            "status": self.status.value,
            "processed_at": self.processed_at.isoformat() if self.processed_at else None,
            "error_message": self.error_message
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ArXivPaper':
        """Create instance from dictionary"""
        authors = [Author(**author_data) for author_data in data.get("authors", [])]
        
        return cls(
            arxiv_id=data["arxiv_id"],
            title=data["title"], 
            abstract=data["abstract"],
            authors=authors,
            categories=data.get("categories", []),
            submitted_date=datetime.fromisoformat(data["submitted_date"]),
            updated_date=datetime.fromisoformat(data["updated_date"]) if data.get("updated_date") else None,
            doi=data.get("doi"),
            journal_ref=data.get("journal_ref"),
            comments=data.get("comments"),
            status=PaperStatus(data.get("status", "raw")),
            processed_at=datetime.fromisoformat(data["processed_at"]) if data.get("processed_at") else None,
            error_message=data.get("error_message")
        )


@dataclass
class ProcessedPaper:
    """Schema for processed paper with cleaned text and extracted features"""
    arxiv_id: str
    cleaned_title: str
    cleaned_abstract: str
    extracted_keywords: List[str]
    citation_count: int = 0
    references: List[str] = field(default_factory=list)  # Referenced paper IDs
    full_text_url: Optional[str] = None
    
    # Text processing metadata
    word_count: int = 0
    language: str = "en"
    readability_score: Optional[float] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "arxiv_id": self.arxiv_id,
            "cleaned_title": self.cleaned_title,
            "cleaned_abstract": self.cleaned_abstract,
            "extracted_keywords": self.extracted_keywords,
            "citation_count": self.citation_count,
            "references": self.references,
            "full_text_url": self.full_text_url,
            "word_count": self.word_count,
            "language": self.language,
            "readability_score": self.readability_score
        }


@dataclass
class PaperEmbedding:
    """Schema for paper embeddings generated by transformer models"""
    arxiv_id: str
    title_embedding: List[float]  # Vector representation of title
    abstract_embedding: List[float]  # Vector representation of abstract
    combined_embedding: List[float]  # Combined title + abstract embedding
    
    # Embedding metadata
    model_name: str  # e.g., "scibert-scivocab-uncased"
    model_version: str
    embedding_dimension: int
    created_at: datetime
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "arxiv_id": self.arxiv_id,
            "title_embedding": self.title_embedding,
            "abstract_embedding": self.abstract_embedding,
            "combined_embedding": self.combined_embedding,
            "model_name": self.model_name,
            "model_version": self.model_version,
            "embedding_dimension": self.embedding_dimension,
            "created_at": self.created_at.isoformat()
        }


@dataclass
class SearchQuery:
    """Schema for user search queries and their processing"""
    query_id: str
    raw_query: str
    processed_query: str
    query_embedding: List[float]
    filters: Dict[str, Any] = field(default_factory=dict)  # date, category, author filters
    
    # Query metadata
    user_id: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "query_id": self.query_id,
            "raw_query": self.raw_query,
            "processed_query": self.processed_query,
            "query_embedding": self.query_embedding,
            "filters": self.filters,
            "user_id": self.user_id,
            "timestamp": self.timestamp.isoformat()
        }


@dataclass
class SearchResult:
    """Schema for search results with relevance scores"""
    arxiv_id: str
    relevance_score: float
    similarity_score: float  # Vector similarity score
    boost_factors: Dict[str, float] = field(default_factory=dict)  # Citation, recency, etc.
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "arxiv_id": self.arxiv_id,
            "relevance_score": self.relevance_score,
            "similarity_score": self.similarity_score,
            "boost_factors": self.boost_factors
        }


@dataclass
class PipelineConfig:
    """Configuration schema for the data processing pipeline"""
    
    # Data sources
    arxiv_api_url: str = "http://export.arxiv.org/api/query"
    arxiv_categories: List[str] = field(default_factory=lambda: ["cs.AI", "cs.CL", "cs.LG", "stat.ML"])
    
    # Processing settings
    batch_size: int = 100
    max_workers: int = 4
    retry_attempts: int = 3
    
    # Embedding model settings
    embedding_model: str = "allenai/scibert_scivocab_uncased"
    embedding_dimension: int = 768
    max_sequence_length: int = 512
    
    # Storage settings
    raw_data_path: str = "./data/raw"
    processed_data_path: str = "./data/processed" 
    embeddings_path: str = "./data/embeddings"
    index_path: str = "./data/index"
    
    # Vector search settings
    faiss_index_type: str = "IVF"
    n_clusters: int = 1024
    similarity_threshold: float = 0.7
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "arxiv_api_url": self.arxiv_api_url,
            "arxiv_categories": self.arxiv_categories,
            "batch_size": self.batch_size,
            "max_workers": self.max_workers,
            "retry_attempts": self.retry_attempts,
            "embedding_model": self.embedding_model,
            "embedding_dimension": self.embedding_dimension,
            "max_sequence_length": self.max_sequence_length,
            "raw_data_path": self.raw_data_path,
            "processed_data_path": self.processed_data_path,
            "embeddings_path": self.embeddings_path,
            "index_path": self.index_path,
            "faiss_index_type": self.faiss_index_type,
            "n_clusters": self.n_clusters,
            "similarity_threshold": self.similarity_threshold
        }


# Data lake/warehouse table schemas for SQL storage
DATABASE_SCHEMAS = {
    "papers": """
    CREATE TABLE IF NOT EXISTS papers (
        arxiv_id VARCHAR(20) PRIMARY KEY,
        title TEXT NOT NULL,
        abstract TEXT NOT NULL,
        authors JSON,
        categories JSON,
        submitted_date TIMESTAMP,
        updated_date TIMESTAMP,
        doi VARCHAR(100),
        journal_ref TEXT,
        comments TEXT,
        status VARCHAR(20) DEFAULT 'raw',
        processed_at TIMESTAMP,
        error_message TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
    );
    """,
    
    "processed_papers": """
    CREATE TABLE IF NOT EXISTS processed_papers (
        arxiv_id VARCHAR(20) PRIMARY KEY,
        cleaned_title TEXT,
        cleaned_abstract TEXT,
        extracted_keywords JSON,
        citation_count INT DEFAULT 0,
        references JSON,
        full_text_url TEXT,
        word_count INT,
        language VARCHAR(10) DEFAULT 'en',
        readability_score FLOAT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (arxiv_id) REFERENCES papers(arxiv_id)
    );
    """,
    
    "embeddings": """
    CREATE TABLE IF NOT EXISTS embeddings (
        arxiv_id VARCHAR(20) PRIMARY KEY,
        title_embedding JSON,
        abstract_embedding JSON,
        combined_embedding JSON,
        model_name VARCHAR(100),
        model_version VARCHAR(50),
        embedding_dimension INT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (arxiv_id) REFERENCES papers(arxiv_id)
    );
    """,
    
    "search_queries": """
    CREATE TABLE IF NOT EXISTS search_queries (
        query_id VARCHAR(50) PRIMARY KEY,
        raw_query TEXT,
        processed_query TEXT,
        query_embedding JSON,
        filters JSON,
        user_id VARCHAR(50),
        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    """
}
